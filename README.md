# ğŸ“Œ PrÃ©sentation du Projet

Ce petit projet explore et manipule des modÃ¨les dâ€™apprentissage automatique sur deux jeux de donnÃ©es : **MNIST** et **Binary AlphaDigits**.  
L'objectif principal est d'Ã©valuer et de comparer les performances de trois architectures de rÃ©seaux de neurones spÃ©cifiques :

### ğŸ”¹ Machines de Boltzmann Restreintes (RBM)
- ModÃ¨les probabilistes non supervisÃ©s pour apprendre des reprÃ©sentations latentes.
- ComposÃ©es d'une couche visible et d'une couche cachÃ©e, sans connexions internes.
- Utilisation de l'algorithme **Contrastive Divergence** pour l'apprentissage.

### ğŸ”¹ Deep Belief Networks (DBN)
- Empilement hiÃ©rarchique de plusieurs RBM pour une prÃ©-initialisation efficace des poids.
- Permet lâ€™extraction de caractÃ©ristiques hiÃ©rarchiques des donnÃ©es.

### ğŸ”¹ Deep Neural Networks (DNN)
- RÃ©seaux de neurones profonds entiÃ¨rement connectÃ©s avec **rÃ©tropropagation**.
- EntraÃ®nement **bout Ã  bout** pour une optimisation directe sur tout le rÃ©seau.

##  MÃ©thodologie

Le projet vise Ã  **comparer ces architectures** en termes de :

-  **CapacitÃ© de reprÃ©sentation** des donnÃ©es.
-  **Performances en classification** sur MNIST et Binary AlphaDigits.
-  **Pouvoir de gÃ©nÃ©ralisation** et **robustesse** face aux variations des donnÃ©es (bruit, transformations).
-  **Temps d'apprentissage** et **analyse des performances** (prÃ©cision, matrice de confusion).

## ğŸ“‚ DonnÃ©es

Les jeux de donnÃ©es utilisÃ©s proviennent de **Kaggle**

